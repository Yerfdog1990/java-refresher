
---

# **Section 2.2: Comparison of Asymptotic Notations**

Asymptotic notations in algorithms are **mathematical tools** used to describe how the performance of an algorithm changes as the input size ( n ) becomes very large.
They donâ€™t give exact runtime in milliseconds; instead, they focus on **growth rate**, which helps us compare algorithms in a way that ignores machine differences.

---

# **1. Why We Use Asymptotic Notations**

They help programmers:

### **1. Analyze algorithm efficiency**

Understand whether an algorithm is fast or slow as input increases.

### **2. Compare different algorithms**

For example, if Algorithm A is ( O(n) ) and Algorithm B is ( O(n^2) ), A will always outperform B on large inputs.

### **3. Predict performance for large datasets**

Very important in data structures, algorithms, system design.

### **4. Ignore machine-specific details**

CPU speed, compiler optimization, RAM sizeâ€”none of these matter with asymptotic analysis.

---

# **2. The Three Primary Asymptotic Notations**

The three most important notations are **Big-O**, **Big-Omega**, and **Big-Theta**.
These notations compare two functions: the algorithmâ€™s runtime ( f(n) ) and a reference growth rate ( g(n) ).

---

## **2.1 Big-O (Upper Bound)**

```
f(n) = O(g(n))
```

Meaning:

> â€œf(n) does not grow faster than g(n) (up to constant factors).â€

This gives the worst-case growth rate.

Example:

```
7n + 10 = O(nÂ²)
```

because ( n^2 ) eventually grows faster than a linear function.

### **Formal definition**

```
f(n) = O(g(n))
if âˆƒ c > 0 and âˆƒ n0 > 0 such that for all n â‰¥ n0:
0 â‰¤ f(n) â‰¤ c * g(n)
```

---

## **2.2 Big-Î© (Lower Bound)**

```
f(n) = Î©(g(n))
```

Meaning:

> â€œf(n) grows at least as fast as g(n).â€

This is the best-case growth rate (or minimum work needed).

Example:

```
nÂ³ - 34 = Î©(10nÂ² - 7n + 1)
```

### **Formal definition**

```
f(n) = Î©(g(n))
if âˆƒ c > 0 and âˆƒ n0 > 0 such that for all n â‰¥ n0:
0 â‰¤ c * g(n) â‰¤ f(n)
```

---

## **2.3 Big-Î˜ (Tight Bound)**

```
f(n) = Î˜(g(n))
```

Meaning:

> â€œf(n) grows at the same rate as g(n).
> Both upper and lower bounds match (up to constants).â€

Example:

```
(1/2)nÂ² - 7n = Î˜(nÂ²)
```

### **Formal definition**

```
f(n) = Î˜(g(n))
if âˆƒ c1, c2 > 0 and âˆƒ n0 > 0 such that for all n â‰¥ n0:
0 â‰¤ c1 * g(n) â‰¤ f(n) â‰¤ c2 * g(n)
```

---

# **3. The Two Secondary Notations: Little-o and Little-Ï‰**

These are stricter versions of Big-O and Big-Î©.

---

## **3.1 Little-o (Strictly Slower)**

```
f(n) = o(g(n))
```

Meaning:

> â€œf(n) grows strictly slower than g(n).â€

Example:

```
5nÂ² = o(nÂ³)
```

### **Formal definition**

```
f(n) = o(g(n))
if for every c > 0, âˆƒ n0 > 0 such that for all n â‰¥ n0:
0 â‰¤ f(n) < c * g(n)
```

---

## **3.2 Little-Ï‰ (Strictly Faster)**

```
f(n) = Ï‰(g(n))
```

Meaning:

> â€œf(n) grows strictly faster than g(n).â€

Example:

```
7nÂ² = Ï‰(n)
```

### **Formal definition**

```
f(n) = Ï‰(g(n))
if for every c > 0, âˆƒ n0 > 0 such that for all n â‰¥ n0:
f(n) > c * g(n)
```

---

# **4. Easy Analogy Using Numbers**

Think of it like comparing two numbers ( a ) and ( b ):

| Notation         | Analogy | Meaning                       |
| ---------------- | ------- | ----------------------------- |
| `f(n) = O(g(n))` | `a â‰¤ b` | f grows no faster than g      |
| `f(n) = Î©(g(n))` | `a â‰¥ b` | f grows at least as fast as g |
| `f(n) = Î˜(g(n))` | `a = b` | f and g grow equally fast     |
| `f(n) = o(g(n))` | `a < b` | f grows strictly slower       |
| `f(n) = Ï‰(g(n))` | `a > b` | f grows strictly faster       |

---

# **5. Why These Notations Matter**

### âœ” They help us focus on growth rate

We ignore tiny differences like +1 or Ã—2 because they donâ€™t matter at scale.

### âœ” They let us compare algorithms fairly

An algorithm with:

* ( O(n) ) will always beat ( O(n^2) ) on large inputs.

### âœ” They are machine-independent

A slow laptop and a fast server both agree that:

* ( O(n \log n) ) grows slower than ( O(n^2) ).

### âœ” They help identify performance bottlenecks

Especially important when datasets grow into the millions.

### âœ” They are essential for writing scalable code

High-performance systems depend on good asymptotic behavior.

---

# **6. Common Complexity Classes (Fast â†’ Slow)**

Below are the most common complexity classes, from best to worst:

```
O(1)        Constant time
O(log n)    Logarithmic
O(n)        Linear
O(n log n)  Linearithmic
O(nÂ²)       Quadratic
O(nÂ³)       Cubic
O(2â¿)       Exponential
O(n!)       Factorial
```

As input size grows:

* ( O(1) ) barely changes
* ( O(n) ) grows steadily
* ( O(n^2) ) becomes slow fast
* ( O(2^n) ) and ( O(n!) ) become impossible to run for large n

---

# **7. Key Properties to Remember**

### âœ” **Drop constants**

```
O(2n) â†’ O(n)
```

### âœ” **Keep only the dominant term**

```
O(nÂ² + n) â†’ O(nÂ²)
```

### âœ” **Asymptotics care about large n**

Small inputs donâ€™t matter to the notation.

### âœ” **Î˜ gives the most accurate description**

Because it includes both upper and lower bounds.

---

# **8. Summary for Absolute Beginners**

* **Big-O** = How fast the algorithm *can* grow (upper bound).
* **Big-Î©** = How fast it *must* grow (lower bound).
* **Big-Î˜** = How fast it *actually* grows (tight bound).
* **Little-o / Ï‰** = Strict versions of Big-O / Big-Î©.
* These notations help compare algorithms without running them.

---

# ğŸ“˜ **Asymptotic Notations â€“ Ultimate Cheat Sheet**

## ğŸ”¹ **What Are Asymptotic Notations?**

Asymptotic notations describe **how the performance of an algorithm grows** with input size **n**.
They ignore hardware differences and focus only on growth rate.

---

# ğŸ“Œ **1. CHEAT SHEET**

![Screenshot 2568-11-16 at 11.47.58.png](Screenshot%202568-11-16%20at%2011.47.58.png)

## **Big-O Notation â€” O(f(n))**

* Describes **upper bound** (worst-case)
* The algorithm will **not grow faster** than this.
* Example:
  `O(n^2)` â†’ runtime grows at most quadratically.

---

## **Big-Omega â€” Î©(f(n))**

* Describes **lower bound** (best-case)
* The algorithm takes **at least** this much time.
* Example:
  `Î©(n log n)` â†’ minimum time is proportional to `n log n`.

---

## **Big-Theta â€” Î˜(f(n))**

* **Tight bound** (both upper & lower)
* Exact growth rate.
* Example:
  `Î˜(n)` â†’ runtime is linear in all cases.

---

## **Little-o â€” o(f(n))**

* **Strictly smaller order** than f(n)
* Algorithm grows *slower* than f(n)
* Example:
  `o(n^2)` â†’ grows slower than quadratic, not equal.

---

## **Little-omega â€” Ï‰(f(n))**

* **Strictly greater order** than f(n)
* Algorithm grows *faster* than f(n)
* Example:
  `Ï‰(n)` â†’ grows faster than linear.

---

# ğŸ“Œ **2. DIAGRAM OF NOTATIONS (ASCII)**

### ğŸ‘‰ Relationship Between Notations

```
           STRICTLY SMALLER            EXACT MATCH             STRICTLY GREATER
    --------------------------------------------------------------------------------
    o(f(n))        O(f(n))       Î˜(f(n))        Î©(f(n))        Ï‰(f(n))    
```

### ğŸ‘‰ Hierarchy of Common Growth Rates

```
O(1)  â†’  O(log n)  â†’  O(n)  â†’  O(n log n)  â†’  O(n^2)  â†’  O(2^n)  â†’  O(n!)
```

### ğŸ‘‰ Visual Growth Curve (conceptual)

```
|                     O(n!)
|                  .
|                .
|            .  
|         .    
|      .
|   .           
| .     O(2^n)
|.  
|      . O(n^2)
|      . 
|      .       O(n log n)
|      .    .
|      .  .
|      ..        O(n)
|      .     O(log n)
|_______._______________________________________ n
        O(1)
```
Here is the scanned table recreated **cleanly in Markdown**, ready to copy and paste:

---

# **Common Complexity Classes**
```markdown
| Name         | Notation      | n = 10        | n = 100               |
|--------------|---------------|---------------|-----------------------|
| Constant     | Î˜(1)          | 1             | 1                     |
| Logarithmic  | Î˜(log(n))     | 3             | 7                     |
| Linear       | Î˜(n)          | 10            | 100                   |
| Linearithmic | Î˜(n * log(n)) | 30            | 700                   |
| Quadratic    | Î˜(n^2)        | 100           | 10 000                |
| Exponential  | Î˜(2^n)        | 1 024         | 1.267650e+30          |
| Factorial    | Î˜(n!)         | 3 628 800     | 9.332622e+157         |
```
---
# ğŸ“Œ **3. MEMORY TRICK (SUPER EASY)**

### ğŸ§  **"SLOTH" â€” Think of a slow animal climbing up the complexity ladder**

Each letter represents a notation and its strength (from weak â†’ strong):

**S â†’ o(f(n))** (S-maller than f(n))
**L â†’ O(f(n))** (L-imit upper bound)
**O â†’ Î˜(f(n))** (O-exact match)
**T â†’ Î©(f(n))** (T-ar minimum)
**H â†’ Ï‰(f(n))** (H-igher than f(n))

ğŸ‘‰ *The SLOTH climbs from smallest to largest.*

```
o  <  O  =  Î˜  <  Î©  <  Ï‰
(S)   (L) (O)  (T)   (H)
```

---

# ğŸ“Œ **4. PRACTICE QUESTIONS (WITH ANSWERS)**

---

### **Q1: What is the Big-O of this loop?**

```java
for (int i = 0; i < n; i++) {
    System.out.println(i);
}
```

**Answer:** `O(n)` â€” runs n times.

---

### **Q2: What is the complexity of nested loops?**

```java
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        System.out.println(i + j);
    }
}
```

**Answer:** `O(n^2)` â€” nested loops.

---

### **Q3: What is the best notation for exact performance?**

âœ” **Î˜(f(n))** â€” the tight bound.

---

### **Q4: True or False**

`o(n)` is the same as `O(n)`.

**Answer:** âŒ False
`o(n)` means *strictly smaller*, `O(n)` allows equality.

---

### **Q5: Which grows faster?**

`n log n` or `n^2`?

**Answer:** `n^2` grows faster.

---

# ğŸ“Œ **5. JAVA EXAMPLES FOR EACH COMPLEXITY CLASS**

---

## **O(1) â€” Constant Time**

```java
int getFirst(int[] arr) {
    return arr[0]; // always constant time
}
```

---

## **O(log n) â€” Logarithmic (Binary Search)**

```java
int binarySearch(int[] arr, int target) {
    int left = 0, right = arr.length - 1;

    while (left <= right) {
        int mid = (left + right) / 2;

        if (arr[mid] == target) return mid;
        if (arr[mid] < target) left = mid + 1;
        else right = mid - 1;
    }
    return -1;
}
```

---

## **O(n) â€” Linear**

```java
int sum(int[] arr) {
    int total = 0;
    for (int n : arr) {
        total += n;
    }
    return total;
}
```

---

## **O(n log n) â€” Sorting**

```java
Arrays.sort(arr); // Built-in merge sort / quicksort hybrid
```

---

## **O(n^2) â€” Quadratic**

```java
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        System.out.println(i + j);
    }
}
```

---

## **O(2^n) â€” Exponential**

```java
int fibonacci(int n) {
    if (n <= 1) return n;
    return fibonacci(n - 1) + fibonacci(n - 2);
}
```

---

## **O(n!) â€” Factorial**

```java
void permutations(String s, String answer) {
    if (s.length() == 0) {
        System.out.println(answer);
        return;
    }
    for (int i = 0; i < s.length(); i++) {
        char ch = s.charAt(i);
        String rest = s.substring(0, i) + s.substring(i + 1);
        permutations(rest, answer + ch);
    }
}
```

---


